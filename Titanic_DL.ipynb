{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Titanic_DL.ipynb","provenance":[{"file_id":"14XQu22L9xzzIE2jgSJ6z_9I2XXEUKmhA","timestamp":1587365354168}],"collapsed_sections":[],"authorship_tag":"ABX9TyPL0QizqIFnuF3JDVNqCe3d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"K5qs9NxAGHIe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607108140357,"user_tz":-180,"elapsed":80244,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}},"outputId":"58c7e4eb-40cd-451b-b4e7-9765af4ea8fc"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 144865 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"urxNyoIeEzuw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607110671597,"user_tz":-180,"elapsed":1350,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}},"outputId":"0c250bcb-0305-4ce1-d269-7984cf5d1f16"},"source":["!mkdir drive\n","!google-drive-ocamlfuse drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘drive’: File exists\n","fuse: mountpoint is not empty\n","fuse: if you are sure this is safe, use the 'nonempty' mount option\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_rpCSVTFIcrL","executionInfo":{"status":"ok","timestamp":1607110674566,"user_tz":-180,"elapsed":813,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}}},"source":["import os \n","os.chdir(\"/content/drive\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3QxJs7eI8J-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607110676798,"user_tz":-180,"elapsed":855,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}},"outputId":"e65a936b-2cb7-4706-f1db-374808987d66"},"source":["import os \n","os.chdir(\"/content/drive/Colab_Notebooks/2december\")\n","!pwd"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/Colab_Notebooks/2december\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nx-iE-OTJa77","executionInfo":{"status":"ok","timestamp":1607110680539,"user_tz":-180,"elapsed":2541,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import csv\n","import re\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import Adam"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLqW-K31mZ5L"},"source":["dtrain = pd.read_csv('train.csv')\n","dtrain.head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtKjo_Q9JhJV","executionInfo":{"status":"ok","timestamp":1607110684020,"user_tz":-180,"elapsed":776,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}}},"source":["\n","# ÖN İŞLEMLER\n","\n","def preprocess(data):\n","    \n","    #convert to char to evaluate into DL Process\n","    data.Cabin.fillna('0', inplace=True)\n","    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n","    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2\n","    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3\n","    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4\n","    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5\n","    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6\n","    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7\n","    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8\n","    \n","    # convert to number to evaluate into DL Process\n","    data['Sex'].replace('female', 1, inplace=True)\n","    data['Sex'].replace('male', 2, inplace=True)\n","    \n","    # convert to char to evaluate into DL Process  \n","    data['Embarked'].replace('S', 1, inplace=True)\n","    data['Embarked'].replace('C', 2, inplace=True)\n","    data['Embarked'].replace('Q', 3, inplace=True)\n","    \n","    #  filling median values for empty age cell.\n","    data['Age'].fillna(data['Age'].median(), inplace=True)\n","    data['Fare'].fillna(data['Fare'].median(), inplace=True)\n","    data['Embarked'].fillna(data['Embarked'].median(), inplace=True)\n","    \n","    # drop not available -- data.dropna\n","    # data.dropna(subset=['Fare', 'Embarked'], inplace=True, how='any')\n","    return data\n","\n","def group_titles(data):\n","    # to define civil status for passengers\n","    data['Names'] = data['Name'].map(lambda x: len(re.split(' ', x)))\n","    data['Title'] = data['Name'].map(lambda x: re.search(', (.+?) ', x).group(1))\n","    data['Title'].replace('Master.', 0, inplace=True)\n","    data['Title'].replace('Mr.', 1, inplace=True)\n","    data['Title'].replace(['Ms.','Mlle.', 'Miss.'], 2, inplace=True)\n","    data['Title'].replace(['Mme.', 'Mrs.'], 3, inplace=True)\n","    data['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 4, inplace=True)\n","\n","def data_subset(data):\n","\n","    features = ['Pclass', 'SibSp', 'Parch', 'Sex', 'Names', 'Title', 'Age', 'Cabin'] #, 'Fare', 'Embarked']\n","    lengh_features = len(features)\n","    subset = data[features]#.fillna(0)\n","    return subset, lengh_features\n","\n","def create_model(train_set_size, input_length, num_epochs, batch_size):\n","    model = Sequential()\n","    model.add(Dense(7, input_dim=input_length, activation='softplus'))\n","    model.add(Dense(3, activation='softplus'))\n","    model.add(Dense(1, activation='softplus'))\n","\n","    lr = .001\n","    adam0 = Adam(lr = lr)\n","\n","    # saving best weights while compiling \n","    model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])\n","    filepath = 'weights.best.hdf5'\n","    checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n","    callbacks_list = [checkpoint]\n","    \n","    history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0) #40, 32\n","    return model, history_model\n","\n","def calculation_storage (history):\n","  temp_loss=[]\n","  temp_acc=[]\n","  loss_history = history.history['loss']\n","  acc_history = history.history['accuracy']\n","  temp_loss=temp_loss+[loss_history]\n","  temp_acc=temp_acc+[acc_history]\n","\n","  return (temp_loss), (temp_acc)\n","\n","def plots(loss_history,acc_history, num_epochs, batchdata):\n","\n","  line_info = batchdata\n","  loss_history = loss_history\n","  acc_history = acc_history\n","  print('koddasın')\n","\n","\n","  epochs = [(i + 1) for i in range(num_epochs)]\n","  count= len(loss_history)\n","  print(loss_history)\n","  print(acc_history)\n","  print(epochs)\n","\n","\n","  ax=plt.subplot(211)\n","  bx=plt.subplot(212)\n","\n","  for k in range((count)):\n","    ax.plot( epochs ,loss_history[k][:],label = line_info[k])\n","  ax.set_xlabel('x - axis')\n","  ax.set_ylabel('y - axis')\n","  ax.set_title('Loss Function vs Epochs')\n","\n","  for k in range((count)):\n","    bx.plot(epochs , acc_history[k][:], label = line_info[k])\n","  bx.set_xlabel('x - axis')\n","  bx.set_ylabel('y - axis')\n","  bx.set_title('Accuracy vs Epochs ')\n","\n","  plt.subplots_adjust(hspace=0.8)\n","  plt.legend(loc='best')\n"," \n","  plt.savefig('Accuracy_loss_batches.png')\n","\n","\n","  plt.show()\n","  plt.close()\n","   \n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0o5Bm-GL5s-","executionInfo":{"status":"ok","timestamp":1607110718085,"user_tz":-180,"elapsed":756,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}}},"source":["def test_method(batch_size):\n","    total_output = []  # assigned for return value\n","    batch_size = batch_size\n","    print(batch_size)\n","    test = pd.read_csv('test.csv', header=0)\n","    test_ids = test['PassengerId']\n","    test = preprocess(test)\n","    group_titles(test)\n","    testdata, _ = data_subset(test)\n","\n","    X_test = np.array(testdata).astype(float)\n","    output = model.predict(X_test, batch_size=batch_size, verbose=0)\n","    output = output.reshape((418,))\n","\n","     #below/above 0,5\n","    outputBin = np.zeros(0)\n","    for element in output:\n","        if element <= .5:\n","             outputBin = np.append(outputBin, False)\n","        else:\n","            outputBin = np.append(outputBin, True)\n","    output1 = np.array(outputBin).astype(bool)\n","\n","\n","    total_output = total_output + [output]\n","    total_output = total_output + [output1]\n","    print(batch_size)\n","    print(output1)\n","    print(batch_size)\n","    print(output1)\n","\n","    return (total_output) \n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqKtFm1Ysq9D","executionInfo":{"status":"ok","timestamp":1607110721296,"user_tz":-180,"elapsed":744,"user":{"displayName":"Mustafa Civici","photoUrl":"","userId":"12844171914551644908"}}},"source":["def csv_writer(txt,batch_rows):\n","  print(txt)\n","  text = txt\n","  array = batch_rows\n","\n","  for j in range(len(text)):\n","    x = text[j]\n","    csv_input = pd.read_csv('myresults.csv')\n","    csv_input[x] = array[j]\n","    csv_input.to_csv('myresults.csv', index=False)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7TCslZIX3Px"},"source":["dtrain.head(10)\n","group_titles(dtrain)\n","dtrain.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwXj54_KJpRu"},"source":["# sonuçların yeniden üretilebilir olması için\n","seed = 7\n","np.random.seed(seed)\n","\n","\n","train = pd.read_csv('train.csv', header=0)\n","\n","\n","preprocess(train)\n","group_titles(train)\n","\n","\n","num_epochs = 150\n","batch_data = [2,4,8,16,32,64]\n","temp_x=[]\n","temp_y=[]\n","\n","\n","traindata, lengh_features = data_subset(train)\n","\n","Y_train = np.array(train['Survived']).astype(int)\n","X_train = np.array(traindata).astype(float)\n","\n","\n","train_set_size = int(.67 * len(X_train))\n","\n","for element in batch_data:\n","  model, history_model = create_model(train_set_size, lengh_features, num_epochs, element)\n","\n","  a = history_model.history['loss']\n","  b = history_model.history['accuracy']\n","  temp_x=temp_x+[a]\n","  temp_y=temp_y+[b]\n","\n","loss_history = temp_x\n","acc_history = temp_y\n","print(loss_history)\n","print(acc_history)\n","\n","plots(loss_history,acc_history,num_epochs,batch_data)\n","\n","\n","X_validation = X_train[train_set_size:]\n","Y_validation = Y_train[train_set_size:]\n","\n","\n","test = pd.read_csv('test.csv', header=0)\n","test_ids = test['PassengerId']\n","column_1 = np.concatenate((['PassengerId'], test_ids ), axis=0 )\n","\n","f = open(\"myresults.csv\", \"w\")\n","writer = csv.writer(f)\n","\n","for i in range(len(test_ids)+1):\n","  writer.writerow([column_1[i]])\n","f.close()\n","\n","batch_rows = []\n","\n","for element in batch_data:\n","  print(\"validasyon part:\")\n","  loss_and_metrics = model.evaluate(X_validation , Y_validation, batch_size=element)\n","  print (\"loss_and_metrics:\", loss_and_metrics)\n","  print(element)\n","  batch_row = test_method(element)\n","  batch_rows = batch_rows + batch_row\n","  #a = a + [a]  #survived\n","  #b = b + [b]  #result\n","  \n","txt_arr=[]\n","  \n","for x in batch_data:\n","  srv_tmp = 'survived - ' + str(x) + 'th epoch'\n","  txt_arr = txt_arr + [srv_tmp]\n","  rst_tmp = 'result - ' + str(x) + 'th epoch'\n","  txt_arr = txt_arr + [rst_tmp]\n","  \n","\n","\n","\n","csv_writer(txt_arr,batch_rows)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hRxU5dYnrrh5"},"source":["Why there is 'batch-size' option in the model.evaluate()\n","\n","\n","\n","Okay, I can see now where your confusion is coming from...\n","\n","For one, some models in Keras require a fixed batch size even during prediction/evaluation. This is true for stateful RNNs, for example, since they keep information from one batch to the next (and the position of a sample within a batch is important!). According to the docs it's also the case for RNNs with Dropout when using the Tensorflow backend, presumably for technical reasons in the implementation.\n","\n","If this doesn't apply to your model, it's true that setting a batch_size isn't technically needed. I believe it can affect the runtime performance of your model though, since it's basically controlling how much data is fed to your GPU at a time (= in one batch).\n","\n","This is how I understand things at least. \n","\n","Comment from: https://github.com/keras-team/keras/issues/3027"]}]}